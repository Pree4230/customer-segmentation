---
title: "HW5"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r }
#Loading the datasets
library(tidyverse)
library(readxl)
library(dplyr)
library(rpart)
library(randomForest)
library(ggplot2)
library(factoextra)
library(scales)
library(cluster)
library(fastDummies)
```

```{r}
#Reading the dataset into R
raw_data = read_excel("/Users/ashritacheetirala/Desktop/UIC/Sem 2/Data Mining/HW5/IMB881-XLS-ENG.xlsx",sheet=2)
```
Preprocessing the data
```{r}
#Removing Unnecessary columns from the dataset
df <- subset (raw_data, select = -c(CustomerOrderNo,Custorderdate,UnitName,TotalArea))
#df

#Converting all binary and categorical  variables to factors
cols <- c("OrderType","OrderCategory","CustomerCode","CountryName","ITEM_NAME","QualityName","DesignName","ColorName","ShapeName")
df[cols]<- lapply(df[cols], factor)
#str(df)
#summary(df)
typeof(df$QtyRequired)
df$QtyRequired <- as.factor(df$QtyRequired)
df$QtyRequired <- as.numeric(df$QtyRequired)
df$Amount <- as.integer(df$Amount)
df$AreaFt <- as.integer(df$AreaFt)
```

**Q1 Visualising the data to provide key insights**
```{r}
#Creating a dataframe to visulaise the revenue generated by various items
group_country <- df %>%                               
  group_by(CountryName) %>% 
  dplyr::summarise(Amount = sum(Amount)) %>%  
  as.data.frame()
#Revenue generated by each country
bar <- ggplot(group_country, aes(fct_reorder(CountryName, Amount),Amount))+
      geom_bar(stat="identity", fill="steelblue")+
      geom_text(aes(label=Amount), vjust=1.6, color="black", size=3.5)+
      coord_flip()+
      ggtitle("Revenue generated by each country")+
      theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
bar
```
Insight: The United States sells the most carpets, followed by the United Kingdom. Other countries that contribute significantly to revenue include Italy, Romania, Australia, India, Canada, and South Africa.

```{r}
#Creating a dataframe to visulaise the revenue generated by various items
grouped_item <- df %>%                               
  group_by(ITEM_NAME) %>% 
  dplyr::summarise(Amount = sum(Amount)) %>%  
  as.data.frame()
#Revenue generated for various types of carpets
pie <- ggplot(grouped_item, aes(x = "", y = Amount, fill = ITEM_NAME)) +
  theme_void()+
  geom_text(aes(label = paste0(round(Amount/sum(Amount)*100), "%")), position = position_stack(vjust = 0.5))+
  geom_bar(width = 1, stat = "identity") + 
  coord_polar(theta = "y", start = 0) +
  ggtitle("Revenue generated from various types of carpets")
pie
```
Insight: With this visualisation we can see that Hand Tufted carpet contributed the highest to the revenue followed by Durry and the lowest being power loom jacquared.

```{r}
#Creating a dataframe to visulaise the revenue generated by various items
group_item <- df %>%                               
  group_by(ITEM_NAME) %>% 
  dplyr::summarise(QtyRequired = sum(QtyRequired)) %>%  
  as.data.frame()
#Number of units of each item sold
col <- group_item %>% 
    ggplot(aes(fct_reorder(ITEM_NAME, QtyRequired),  QtyRequired))+
  geom_col(fill="gold") +
  labs(x="Name of Item")+
  geom_text(aes(label=QtyRequired), vjust=1.6, color="black", size=3.5)+
  theme(axis.text.y=element_blank(),axis.ticks.y=element_blank())+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
  ggtitle("Number of units sold of each item")
col
```
Insight: The "Durry" type carpets were sold the most, but it was the second highest contributor of revenue and significantly lower than "Hand Tufted" in terms of revenue, so we can say it is much cheaper in price. Because "Hand Tufted" carpet is the highest revenue generator but the number of units sold is much lower, we can say that it is on the expensive side and thus a premium quality carpet.

**Q2**
Champo Carpets can use various ML algorithms to solve their problem. Champo Carpet's main aim is to reduce the number of false positives. When the order is actually not converted, but they are predicted as converted, there will be loss for Champo carpets as the samples made are wasted and it is expensive to make each sample. Hence, the champo carpets must be focusing on reducing the precision(actually not converted but predicted as converted).

All the ML classification algorithms like decision trees, randoForest can be used to find out the precision of test data. When it comes to improving the precision, it is better to use randomForest than decision trees as it prevents overfitting by using multiple trees. Neural networks can also be used to predict the future data as it discovers any complex relations hidden in the data. 

With regression, we can predict the output based on input variables. We can find out the importance of the variables by checking if they are statistically significant or not. Logistic regression can be performed to understand the relationship between predictor variables and probability of orders getting converted to samples.


**Q3 Various ML Models on Balanced and imbalanced data**
```{r}
data_sample = read_excel("/Users/ashritacheetirala/Desktop/UIC/Sem 2/Data Mining/HW5/IMB881-XLS-ENG.xlsx",sheet=4)
library('fastDummies')
data_sample <- dummy_cols(data_sample, select_columns = 'CountryName')
```

```{r}
#data cleaning 
data_sample_decion_tree <- subset(data_sample,select = -c(USA,UK,Italy,Belgium,Romania,Australia,India,`Hand Tufted`,Durry,`Double Back`,`Hand Woven`,Knotted,Jacquard,Handloom,Other,REC,Round,Square,CountryName_AUSTRALIA,CountryName_BELGIUM,CountryName_BRAZIL,CountryName_CANADA,CountryName_CHINA,CountryName_INDIA,CountryName_ISRAEL,CountryName_ITALY,CountryName_POLAND,CountryName_ROMANIA,`CountryName_SOUTH AFRICA`,CountryName_UAE,CountryName_UK,CountryName_USA,CustomerCode))
str(data_sample_decion_tree)

data_sample_decion_tree$`Order Conversion` <- ifelse(data_sample_decion_tree$`Order Conversion`==1,"Yes","No")
df1<-data_sample_decion_tree
```

```{r}
cols <- c("CountryName","ITEM_NAME","ShapeName","Order Conversion")
df1[cols]<- lapply(df1[cols], factor)
str(df1)
```

```{r}
#decision tree
set.seed(60) 
indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.5,0.5))#dividing the dataset into training and test with 50% in train and 50% in test 
train <-df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <- df1 [indx==2, ] #assigning all the rows with index 2 to test 
library("rpart.plot") 
tree_m1 <- rpart(`Order Conversion` ~ ., train, parms = list(split = "gini" )) #constructing the decision tree using rpart print( tree_m1) #printing the decision tree
print(tree_m1)
plotcp(tree_m1)
rpart.plot(tree_m1)

tree_pred_class_1 <- predict(tree_m1, train, type = "class")#using predict function to predict the classes of training data 
trainerror_1 <- mean(tree_pred_class_1 != train$`Order Conversion`) #calculating the training error
trainerror_1

tree_pred_test_1 <- predict(tree_m1, newdata=test, type = "class")#using predict function to predict the classes of test data 
testerror_1 <- mean(tree_pred_test_1 != test$`Order Conversion`) #calculating the test error
testerror_1

difference <- testerror_1 - trainerror_1
difference

CM <- table(tree_pred_test_1,test$`Order Conversion`) 
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test
prop.table(table(df1$`Order Conversion`))

```

```{r}
minsplt <- c(15, 51, 104) #assigning random vector values to minsplit 
minbckt <- c(5, 17, 38) #assigning random vector values to minbckt 
#looping through to try different combinations of minsplit and minbucket 
for (i in minsplt){
for (j in minbckt){ 
tree_m1 <- rpart(`Order Conversion` ~ ., train, parms = list(split = "gini" ), control = rpart.control(minbucket = j, minsplit =i, cp=0.01))
tree_pred_class_1 <- predict(tree_m1, train, type = "class")#using predict function to predict the classes of training data 
trainerror_1 <- mean(tree_pred_class_1 != train$`Order Conversion`) #calculating the training error 
tree_pred_test_1 <- predict(tree_m1, test, type = "class")#using predict function to predict the classes of test data
testerror_1 <- mean(tree_pred_test_1 != test$`Order Conversion`) #calculating the test error 
dif <- testerror_1-trainerror_1 #finding out the difference between test error and training error 
CM <- table(tree_pred_test_1, test$`Order Conversion`)
print(CM) 
#Assigning the values of matrix to the following variables
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
}}
```

```{r}
#decision tree
set.seed(60) 
indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.7,0.3))#dividing the dataset into training and test with 50% in train and 50% in test 
train <-df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <- df1 [indx==2, ] #assigning all the rows with index 2 to test 
library("rpart.plot") 
tree_m2 <- rpart(`Order Conversion` ~ ., train, parms = list(split = "gini" )) #constructing the decision tree using rpart print( tree_m2) #printing the decision tree
print(tree_m2)
plotcp(tree_m2)
rpart.plot(tree_m2)
plotcp(tree_m2)
printcp(tree_m2)

tree_pred_class_2 <- predict(tree_m2, train, type = "class")#using predict function to predict the classes of training data 
trainerror_2 <- mean(tree_pred_class_2 != train$`Order Conversion`) #calculating the training error
trainerror_2

tree_pred_test_2 <- predict(tree_m2, newdata=test, type = "class")#using predict function to predict the classes of test data 
testerror_2 <- mean(tree_pred_test_2 != test$`Order Conversion`) #calculating the test error
testerror_2

difference <- testerror_2 - trainerror_2
difference

CM <- table(tree_pred_test_2,test$`Order Conversion`) 
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
```

```{r}
minsplt <- c(15, 51, 104) #assigning random vector values to minsplit 
minbckt <- c(5, 17, 38) #assigning random vector values to minbckt 
#looping through to try different combinations of minsplit and minbucket 
for (i in minsplt){
for (j in minbckt){ 
tree_m2 <- rpart(`Order Conversion` ~ ., train, parms = list(split = "gini" ), control = rpart.control(minbucket = j, minsplit =i, cp=0.01))
tree_pred_class_2 <- predict(tree_m2, train, type = "class")#using predict function to predict the classes of training data 
trainerror_2 <- mean(tree_pred_class_2 != train$`Order Conversion`) #calculating the training error 
tree_pred_test_2 <- predict(tree_m2, test, type = "class")#using predict function to predict the classes of test data
testerror_2 <- mean(tree_pred_test_2 != test$`Order Conversion`) #calculating the test error 
dif <- testerror_2-trainerror_2 #finding out the difference between test error and training error 
CM <- table(tree_pred_test_2, test$`Order Conversion`)
print(CM) 
#Assigning the values of matrix to the following variables
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
}}
```

```{r}
#decision tree
set.seed(60) 
indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 50% in train and 50% in test 
train <- df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <- df1 [indx==2, ] #assigning all the rows with index 2 to test 
library("rpart.plot") 
tree_m3 <- rpart(`Order Conversion` ~ ., train, parms = list(split = "gini" )) #constructing the decision tree using rpart print( tree_m2) #printing the decision tree
print(tree_m3)
rpart.plot(tree_m3)
plotcp(tree_m3)
printcp(tree_m3)

tree_pred_class_3 <- predict(tree_m3, train, type = "class")#using predict function to predict the classes of training data 
trainerror_3 <- mean(tree_pred_class_3 != train$`Order Conversion`) #calculating the training error
trainerror_3

tree_pred_test_3 <- predict(tree_m3, newdata=test, type = "class")#using predict function to predict the classes of test data 
testerror_3 <- mean(tree_pred_test_3 != test$`Order Conversion`) #calculating the test error
testerror_3

difference <- testerror_3 - trainerror_3
difference

CM <- table(tree_pred_test_3,test$`Order Conversion`) 
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
```

```{r}
minsplt <- c(15, 51, 104) #assigning random vector values to minsplit 
minbckt <- c(5, 17, 38) #assigning random vector values to minbckt 
#looping through to try different combinations of minsplit and minbucket 
for (i in minsplt){
for (j in minbckt){ 
tree_m3 <- rpart(`Order Conversion` ~ ., train, parms = list(split = "gini" ), control = rpart.control(minbucket = j, minsplit =i, cp=0.01))
tree_pred_class_3 <- predict(tree_m2, train, type = "class")#using predict function to predict the classes of training data 
trainerror_3 <- mean(tree_pred_class_3 != train$`Order Conversion`) #calculating the training error 
tree_pred_test_3 <- predict(tree_m3, test, type = "class")#using predict function to predict the classes of test data
testerror_3 <- mean(tree_pred_test_3 != test$`Order Conversion`) #calculating the test error 
dif <- testerror_3-trainerror_3 #finding out the difference between test error and training error 
CM <- table(tree_pred_test_3, test$`Order Conversion`)
print(CM) 
#Assigning the values of matrix to the following variables
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
}}
```
When the order is actually not converted, but they are predicted as converted, there will be loss for Champo carpets as the samples made are wasted. Hence, we chose precision as our performance metric as it is crucial to reduce the False positives(actually not converted but predicted as converted)

According to the decision trees that have been constructed above, we can conclude that 70:30 split with the highlighted pruning parameters gives us the best recall.

Constructing random forests with different ntree values and finding the best mtry for each model to derive a single model with the best performance
```{r}
#Random Forest Model
rf <- randomForest(`Order Conversion` ~ ., data = df1, mtry = sqrt(ncol(df1)-1), ntree = 100, proximity = T, importance = T)
imp_variables<-importance(rf, type = 2) 
imp_variables   
#Model1
set.seed(60) 
indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <- df1 [indx==2, ] #assigning all the rows with index 2 to test
pr.err <- c() 
for(mt in seq(1,ncol(train))) { 
 rf1 <- randomForest(`Order Conversion`~., data = train, 
       ntree = 100, mtry = ifelse(mt == ncol(train), mt-1, mt))
predicted <- predict(rf1, newdata = test, type = "class") 
pr.err <- c(pr.err,mean(test$`Order Conversion` != predicted))
}
bestmtry <- which.min(pr.err) 
print(bestmtry)

rf1 <- randomForest(`Order Conversion`~., data = train, ntree = 100, mtry =bestmtry)
print(rf1)

predicted <- predict(rf1, newdata = test, type = "class")
CM <- table(predicted, test$`Order Conversion`)
print(CM)
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1]
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test

#Model2
set.seed(60) 
indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <- df1 [indx==2, ] #assigning all the rows with index 2 to test
pr.err <- c() 
for(mt in seq(1,ncol(train))) { 
 rf1 <- randomForest(`Order Conversion`~., data = train, 
       ntree = 300, mtry = ifelse(mt == ncol(train), mt-1, mt))
predicted <- predict(rf1, newdata = test, type = "class") 
pr.err <- c(pr.err,mean(test$`Order Conversion` != predicted))
}
bestmtry <- which.min(pr.err) 
print(bestmtry)

rf2 <- randomForest(`Order Conversion`~., data = train, ntree = 100, mtry =bestmtry)
print(rf2)

predicted <- predict(rf2, newdata = test, type = "class")
CM <- table(predicted, test$`Order Conversion`)
print(CM)
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1]
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test


```
According to the randomforest models constructed above, the randomforest model1 is considered as it gives us the better precision.

We can also determine the important variables by looking at the gini reduction of each variable. We can see that AreaFt has highest gini reduction and hence the most important variable.

```{r}
#Logistic Regression

set.seed(60) 

indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <-df1 [indx==2, ] #assigning all the rows with index 2 to test

logitModel <- glm(`Order Conversion` ~ ., data = train, family = "binomial")
summary(logitModel)

anova(logitModel, test="Chisq")
Pred <- predict(logitModel, newdata = test, type = "response")
err<-mean(Pred != test$`Order Conversion`)
err
Class <- ifelse(Pred >= 0.5, "YES", "NO")

with(logitModel, null.deviance - deviance) 

with(logitModel, df.null, df.residual)

with(logitModel, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))


```
According to the P values that we have got above, we can say that the variables
CountryNameBELGIUM, CountryNameCANADA, CountryNameROMANIA, ITEM_NAMEGUN TUFTED, ITEM_NAMEHANDWOVEN, ITEM_NAMEKNOTTED, ITEM_NAMEPOWER LOOM JACQUARD, TEM_NAMETABLE TUFTED, AreaFt are significant as they are less than alpha (1%).

The anova table gives the residual deviance of null model and other variables. The more the difference in deviance between the null and residual, the best our model is doing against the null model. Hence, in the above table we can see that adding CountryName reduces the deviance. 


```{r}
#Neural network
library(dplyr)
myscale <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
df1 <- df1 %>% mutate_if(is.numeric, myscale)

indx <- sample(2, nrow(df1), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- df1 [indx==1, ] #assigning all the rows with index 1 to train 
test <-df1 [indx==2, ] #assigning all the rows with index 2 to test

library(nnet)
nnModel <- nnet(`Order Conversion` ~ ., data = train, linout = FALSE,
                size = 10, decay = 0.01, maxit = 500)

summary(nnModel)

#nnModel$wts
#nnModel$fitted.values

#install.packages("NeuralNetTools")
library(NeuralNetTools)
plotnet(nnModel)

nn.preds = predict(nnModel, test)

nn.preds = as.factor(predict(nnModel, test, type = "class"))


CM <- table(nn.preds, test$`Order Conversion`)
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test

#decay parameter
set.seed(60)
indx <- sample(2, nrow(train), replace = T, prob = c(0.5, 0.5))
train2 <- train[indx == 1, ]
validation <- train[indx == 2, ]

err <- vector("numeric", 100)
d <- seq(0.0001, 1, length.out=100)
k = 1
for(i in d) {
  mymodel <- nnet(`Order Conversion` ~., data = train2, decay = i, size = 10, maxit = 1000)
  pred.class <- predict(mymodel, newdata = validation, type = "class")
  err[k] <- mean(pred.class != validation$`Order Conversion`)
  k <- k +1
}
plot(d, err)
```

Balancing the data
```{r}
library(ROSE)
colnames(df1)[which(names(df1) == "Order Conversion")] <- "Target"

#balancing the data
balanced_data <- ovun.sample(Target~.,data = df1,method = "over",N = 9600)$data
summary(balanced_data$Target)
```

```{r}
#decision tree
set.seed(60) 
indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.5,0.5))#dividing the dataset into training and test with 50% in train and 50% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <- balanced_data [indx==2, ] #assigning all the rows with index 2 to test 
library("rpart.plot") 
tree_m1 <- rpart(Target ~ ., train, parms = list(split = "gini" )) #constructing the decision tree using rpart print( tree_m1) #printing the decision tree
print(tree_m1)
plotcp(tree_m1)
rpart.plot(tree_m1)

tree_pred_class_1 <- predict(tree_m1, train, type = "class")#using predict function to predict the classes of training data 
trainerror_1 <- mean(tree_pred_class_1 != train$Target) #calculating the training error
trainerror_1

tree_pred_test_1 <- predict(tree_m1, newdata=test, type = "class")#using predict function to predict the classes of test data 
testerror_1 <- mean(tree_pred_test_1 != test$Target) #calculating the test error
testerror_1

difference <- testerror_1 - trainerror_1
difference

CM <- table(tree_pred_test_1,test$Target) 
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test

```


```{r}
minsplt <- c(15, 51, 104) #assigning random vector values to minsplit 
minbckt <- c(5, 17, 38) #assigning random vector values to minbckt 
#looping through to try different combinations of minsplit and minbucket 
for (i in minsplt){
for (j in minbckt){ 
tree_m1 <- rpart(Target ~ ., train, parms = list(split = "gini" ), control = rpart.control(minbucket = j, minsplit =i, cp=0.01))
tree_pred_class_1 <- predict(tree_m1, train, type = "class")#using predict function to predict the classes of training data 
trainerror_1 <- mean(tree_pred_class_1 != train$Target) #calculating the training error 
tree_pred_test_1 <- predict(tree_m1, test, type = "class")#using predict function to predict the classes of test data
testerror_1 <- mean(tree_pred_test_1 != test$Target) #calculating the test error 
dif <- testerror_1-trainerror_1 #finding out the difference between test error and training error 
CM <- table(tree_pred_test_1, test$Target)
print(CM) 
#Assigning the values of matrix to the following variables
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
}}
```

```{r}
#decision tree
set.seed(60) 
indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.7,0.3))#dividing the dataset into training and test with 50% in train and 50% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <- balanced_data [indx==2, ] #assigning all the rows with index 2 to test 
library("rpart.plot") 
tree_m2 <- rpart(Target ~ ., train, parms = list(split = "gini" )) #constructing the decision tree using rpart print( tree_m2) #printing the decision tree
print(tree_m2)
plotcp(tree_m2)
rpart.plot(tree_m2)
printcp(tree_m2)

tree_pred_class_2 <- predict(tree_m2, train, type = "class")#using predict function to predict the classes of training data 
trainerror_2 <- mean(tree_pred_class_2 != train$Target) #calculating the training error
trainerror_2

tree_pred_test_2 <- predict(tree_m2, newdata=test, type = "class")#using predict function to predict the classes of test data 
testerror_2 <- mean(tree_pred_test_2 != test$Target) #calculating the test error
testerror_2

difference <- testerror_2 - trainerror_2
difference

CM <- table(tree_pred_test_2,test$Target) 
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
```

```{r}
minsplt <- c(15, 51, 104) #assigning random vector values to minsplit 
minbckt <- c(5, 17, 38) #assigning random vector values to minbckt 
#looping through to try different combinations of minsplit and minbucket 
for (i in minsplt){
for (j in minbckt){ 
tree_m2 <- rpart(Target ~ ., train, parms = list(split = "gini" ), control = rpart.control(minbucket = j, minsplit =i, cp=0.01))
tree_pred_class_2 <- predict(tree_m2, train, type = "class")#using predict function to predict the classes of training data 
trainerror_2 <- mean(tree_pred_class_2 != train$Target) #calculating the training error 
tree_pred_test_2 <- predict(tree_m2, test, type = "class")#using predict function to predict the classes of test data
testerror_2 <- mean(tree_pred_test_2 != test$Target) #calculating the test error 
dif <- testerror_2-trainerror_2 #finding out the difference between test error and training error 
CM <- table(tree_pred_test_2, test$Target)
print(CM) 
#Assigning the values of matrix to the following variables
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
}}
```

```{r}
#decision tree
set.seed(60) 
indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 50% in train and 50% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <- balanced_data [indx==2, ] #assigning all the rows with index 2 to test 
library("rpart.plot") 
tree_m3 <- rpart(Target ~ ., train, parms = list(split = "gini" )) #constructing the decision tree using rpart print( tree_m2) #printing the decision tree
print(tree_m3)
rpart.plot(tree_m3)
plotcp(tree_m3)
printcp(tree_m3)

tree_pred_class_3 <- predict(tree_m3, train, type = "class")#using predict function to predict the classes of training data 
trainerror_3 <- mean(tree_pred_class_3 != train$Target) #calculating the training error
trainerror_3

tree_pred_test_3 <- predict(tree_m3, newdata=test, type = "class")#using predict function to predict the classes of test data 
testerror_3 <- mean(tree_pred_test_3 != test$Target) #calculating the test error
testerror_3

difference <- testerror_3 - trainerror_3
difference

CM <- table(tree_pred_test_3,test$Target) 
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
```

```{r}
minsplt <- c(15, 51, 104) #assigning random vector values to minsplit 
minbckt <- c(5, 17, 38) #assigning random vector values to minbckt 
#looping through to try different combinations of minsplit and minbucket 
for (i in minsplt){
for (j in minbckt){ 
tree_m3 <- rpart(Target ~ ., train, parms = list(split = "gini" ), control = rpart.control(minbucket = j, minsplit =i, cp=0.01))
tree_pred_class_3 <- predict(tree_m2, train, type = "class")#using predict function to predict the classes of training data 
trainerror_3 <- mean(tree_pred_class_3 != train$Target) #calculating the training error 
tree_pred_test_3 <- predict(tree_m3, test, type = "class")#using predict function to predict the classes of test data
testerror_3 <- mean(tree_pred_test_3 != test$Target) #calculating the test error 
dif <- testerror_3-trainerror_3 #finding out the difference between test error and training error 
CM <- table(tree_pred_test_3, test$Target)
print(CM) 
#Assigning the values of matrix to the following variables
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
print(precision_test)
}}
```

We can observe from the precision values that we got, that balanced data gives us better precision than unbalanced data. 

We can observe from the above table that the 80:20 split gives us the best precision.
```{r}
#Random Forest Model
set.seed(60) 
indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <- balanced_data [indx==2, ] #assigning all the rows with index 2 to test
pr.err <- c() 
for(mt in seq(1,ncol(train))) { 
 rf1 <- randomForest(Target~., data = train, 
       ntree = 100, mtry = ifelse(mt == ncol(train), mt-1, mt))
predicted <- predict(rf1, newdata = test, type = "class") 
pr.err <- c(pr.err,mean(test$Target != predicted))
}
bestmtry <- which.min(pr.err) 
print(bestmtry)

rf1 <- randomForest(Target~., data = train, ntree = 100, mtry =bestmtry)
print(rf1)

predicted <- predict(rf1, newdata = test, type = "class")
CM <- table(predicted, test$Target)
print(CM)
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1]
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test


set.seed(60) 
indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <- balanced_data [indx==2, ] #assigning all the rows with index 2 to test
pr.err <- c() 
for(mt in seq(1,ncol(train))) { 
 rf1 <- randomForest(Target~., data = train, 
       ntree = 300, mtry = ifelse(mt == ncol(train), mt-1, mt))
predicted <- predict(rf1, newdata = test, type = "class") 
pr.err <- c(pr.err,mean(test$Target != predicted))
}
bestmtry <- which.min(pr.err) 
print(bestmtry)

rf2 <- randomForest(Target~., data = train, ntree = 100, mtry =bestmtry)
print(rf2)

predicted <- predict(rf2, newdata = test, type = "class")
CM <- table(predicted, test$Target)
print(CM)
TN =CM[1,1] 
TP =CM[2,2] 
FP =CM[1,2] 
FN =CM[2,1]
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test


```

There is a drastic improvement in precision when balanced data is used for randomforest. On balanced data, the randomForest Model2 with 300 trees gives better precision 

```{r}
#Logistic Regression

set.seed(60) 

indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <-balanced_data [indx==2, ] #assigning all the rows with index 2 to test

logitModel <- glm(Target ~ ., data = train, family = "binomial")
summary(logitModel)
confint(logitModel)

with(logitModel, null.deviance - deviance) 

with(logitModel, df.null, df.residual)

with(logitModel, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))

```
When compared to unbalanced data, balanced data gives us more number of significant variables. Also, the balanced data gives better AIC than unbalanced data.

```{r echo=FALSE}
#Neural network
library(dplyr)
myscale <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
balanced_data <- balanced_data %>% mutate_if(is.numeric, myscale)

indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <-balanced_data [indx==2, ] #assigning all the rows with index 2 to test

library(nnet)
nnModel <- nnet(Target ~ ., data = train, linout = FALSE,
                size = 10, decay = 0.01, maxit = 500)

summary(nnModel)

nnModel$wts
nnModel$fitted.values

#install.packages("NeuralNetTools")
library(NeuralNetTools)
plotnet(nnModel)

nn.preds = predict(nnModel, test)

nn.preds = as.factor(predict(nnModel, test, type = "class"))


CM <- table(nn.preds, test$Target)
print(CM)

#Assigning the values of matrix to the following variables 
TN =CM[1,1] 
TP =CM[2,2]
FP =CM[1,2] 
FN =CM[2,1] 
precision_test =(TP)/(TP+FP) #calculating precision of test data
precision_test

#decay parameter
set.seed(60)
indx <- sample(2, nrow(train), replace = T, prob = c(0.5, 0.5))
train2 <- train[indx == 1, ]
validation <- train[indx == 2, ]

err <- vector("numeric", 100)
d <- seq(0.0001, 1, length.out=100)
k = 1
for(i in d) {
  mymodel <- nnet(Target ~., data = train2, decay = i, size = 10, maxit = 1000)
  pred.class <- predict(mymodel, newdata = validation, type = "class")
  err[k] <- mean(pred.class != validation$Target)
  k <- k +1
}
plot(d, err)
```
The neural network done on balanced data gives better precision than unbalanced data. 

```{r}
#Ada boosting
set.seed(60) 

indx <- sample(2, nrow(balanced_data), replace=TRUE, prob=c(0.8,0.2))#dividing the dataset into training and test with 80% in train and 20% in test 
train <- balanced_data [indx==1, ] #assigning all the rows with index 1 to train 
test <-balanced_data [indx==2, ] #assigning all the rows with index 2 to test


library(adabag)
model <- boosting(Target~., data=train, boos=TRUE, mfinal=10)
print(names(model))
print(model$trees[1])

```

**Q4. Data strategy**  
Data Preprocessing
We consider the "data for clustering" sheet for customer segmentation using clustering.As a first step to preprocessing, since categorical variables cannot be used in the clustering algorithms we remove the row labels column. To be more specific, the range of categorical variables (e.g. row variables in this data) is discrete (one of the customers name), hence cannot be directly combined with a continuous variable and measured the distance in the same manner.
Since any clustering algorithm interpret the closeness between data points based on a distance measure, it is important to reconcile all dimensions into a standard scale. An appropriate type of data transformation should be selected to align with the distribution of the data. For the case of this dataset we standardize all the variables between 0 and 1. With the variables we have, we can divide the dataset into two subsets. One which will have the variables Sum of QtyRequired, Sum of TotalArea and Sum of Amount. The other variables will be part of an other subset. This way we can cluster the similar customers in a better way. We can use principal component analysis to reduce the dimensions in the case of second subset. We'll also use elbow method to determine the number of actual clusters.

Benefit to business
Clustering algorithm helps to better understand customers. Customer with comparable characteristics often have similar interest, thus business can benefit from this technique by creating tailored samples for each customer segment.Determine appropriate product pricing, Design an optimal distribution strategy can be the other benefits.

**Q5. Clustering Algorithms**

For globular shaped clusters, center-based alogrithms (K means) are more adaptable where as if the cluster are irregular in shape and have a lot of noise then density based algorithms (DBSCAN) are more applicable. Since we have reduced the dimensions of the datasets as explained in Q4 using K means clustering algorithm will give us the desired results. K means algorithm uses the euclidean distance to interpret the closeness between data points. The number of clusters K can be found out using the elbow method.

**Q6. Using K-means to solve the problem**
```{r}
champo_carpets1 = read_excel("/Users/ashritacheetirala/Desktop/UIC/Sem 2/Data Mining/HW5/IMB881-XLS-ENG.xlsx",sheet=6)
```

```{r}
#Remove the character variable column and also standardize all the remaining columns
champo_carpets = subset(champo_carpets1, select = -c(1) )
champo_carpets = sapply(champo_carpets, rescale)
```

```{r}
#Divide the dataframe to two subsets for k means clustering
carpets_sum = subset(champo_carpets, select = c(1:3))
carpets_type = subset(champo_carpets, select = c(4:13))
```

```{r}
#Use pca for carpets_type subset to extract the principal features and discard the remaining
carpets_type.pca <- prcomp(carpets_type, center = TRUE,scale. = TRUE)
summary(carpets_type.pca)
```
From summary it is evident that we can capture 85% of the information in the dataset (10 variables) can be encapsulated by just the first 5 Principal Components.

```{r}
#pick the principle components from summary
carpets_transform = as.data.frame(-carpets_type.pca$x[,1:5])
```

```{r}
#Consider the number of clusters for the carpets type dataset
fviz_nbclust(carpets_transform, kmeans, method = 'wss')
```

```{r}
#Consider the number of clusters for the carpets sum dataset
fviz_nbclust(carpets_sum, kmeans, method = 'wss')

```
In both the cases we can see that there is no significant difference in the sum of squares from k value equal to 6. Hence we choose the number of clusters to be 6.

```{r}
#Applying k-means on carpets_transform datset
kmeans_type = kmeans(carpets_transform, centers = 6, nstart = 100)
fviz_cluster(kmeans_type, data = carpets_transform)
rownames(carpets_transform) <- champo_carpets1$`Row Labels`
```

```{r}
#Applying k-means on carpets_transform datset
kmeans_sum = kmeans(carpets_sum, centers = 6, nstart = 100)
fviz_cluster(kmeans_sum, data = carpets_sum)
rownames(carpets_sum) <- champo_carpets1$`Row Labels`
```

We can see that majority of the customers belong to a single cluster in both the cases. The clustering is also very similar between the two graphs thus strengthening the confidence of it.

**Q7. Recommender System**
```{r}
recommendation <- read_excel("/Users/ashritacheetirala/Desktop/UIC/Sem 2/Data Mining/HW5/IMB881-XLS-ENG.xlsx",sheet=5)
recommend <- recommendation[-1]
View(recommend)
rownames(recommend) <- recommendation$Customer
rec_mat <- as.matrix(recommend)
sim_mat <- cor(t(rec_mat), method="pearson")
sim_mat
```
Some of the recommendations can be made to the customers are:

Using the correlation matrix we can see that customers N1 and C1 are similar to each other as they have 97% correlation. Therefeore after constructing the recommender matrix we can recommend N1 to order Knotted carpets andf double woven carpets in neutral shades. 

We can also see that T-5 and PD are similar as they have 99% correlation and we can recommend Hand Tufted carpets to PD that are round in shape and in shades of pink and blush pink. 

Another set of similar customers are PC and CC as they have 99% and we can recommend handloom carpets to PC that are round and in shades of navy and blue.

**Association rules(extra credit)**
```{r}
library(arules)
library(arulesViz)
ass <- read_excel("/Users/ashritacheetirala/Desktop/UIC/Sem 2/Data Mining/HW5/IMB881-XLS-ENG.xlsx",sheet=7)
ass <- ass[2:8]
colnames(ass)
ass <- ass %>% mutate_if(is.numeric,as.character)
ass <- ass %>% mutate_if(is.character,as.factor)
rules <- apriori(ass, parameter = list(supp=0.001, minlen=3, maxlen=5,conf=0.08))
rules_conf <- sort (rules, by="confidence", decreasing=TRUE)
inspect(rules_conf[1:10])
plot(rules,jitter=0)
```

**Q8 Final Recommendations**

Our final recommedation to Champo carpets would be to use all these different models as they provide different insights. 
- We can suggest them the important factors that led to conversion of orders are AreaFt, CountryName,QtyRequired   as we have seen the ANOVA table of logistic regression predicting them as significant features. 
- We can use the recommender systems to recommend products to customers depending on their similarity with the    other customers while Kmeans can help us understand various segments of customers that we have and then make    better strategies to increase their conversion rate thereby targeting the customers. 
- Using Association rules we can also suggest them the items that go well with their purchase history.
- The company can also prioritize using balanced data as it can give more accurate and precise results.

